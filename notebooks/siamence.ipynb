{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_in_batches(file_path: str, batch_size=100000):\n",
    "\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    processed_rows = 0\n",
    "\n",
    "    for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "        batch_df = batch.to_pandas()\n",
    "\n",
    "        processed_rows += len(batch_df)\n",
    "        progress = (processed_rows / total_rows) * 100\n",
    "        print(f'Progress: {progress:.2f}%')\n",
    "        \n",
    "        yield batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 8.56%\n",
      "Progress: 17.12%\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "batches = []\n",
    "for batch in read_parquet_in_batches(file_path='../data/train/train_data_validate.parquet'):\n",
    "    if not batches:\n",
    "        batches.append(batch)\n",
    "    else:\n",
    "        batches.append(batch)\n",
    "        break\n",
    "    \n",
    "for batch in batches:\n",
    "    batch.drop(columns=['variantid1', 'variantid2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding1</th>\n",
       "      <th>embedding2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.5318107604980469, 0.35363996028900146, -0.7...</td>\n",
       "      <td>[0.5318107604980469, 0.35363996028900146, -0.7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.4308440089225769, 0.7620932459831238, 0.793...</td>\n",
       "      <td>[0.5668608546257019, 0.9573432803153992, 1.017...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.36238163709640503, 0.4316844344139099, -0....</td>\n",
       "      <td>[-0.25123998522758484, 0.3757574260234833, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.7327960729598999, -0.7488707900047302, 0.55...</td>\n",
       "      <td>[0.7327960729598999, -0.7488707900047302, 0.55...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-1.3140270709991455, -0.8071212768554688, 0.7...</td>\n",
       "      <td>[-0.49589139223098755, -0.5760805606842041, 0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          embedding1  \\\n",
       "0  [0.5318107604980469, 0.35363996028900146, -0.7...   \n",
       "1  [0.4308440089225769, 0.7620932459831238, 0.793...   \n",
       "2  [-0.36238163709640503, 0.4316844344139099, -0....   \n",
       "3  [0.7327960729598999, -0.7488707900047302, 0.55...   \n",
       "4  [-1.3140270709991455, -0.8071212768554688, 0.7...   \n",
       "\n",
       "                                          embedding2  target  \n",
       "0  [0.5318107604980469, 0.35363996028900146, -0.7...       1  \n",
       "1  [0.5668608546257019, 0.9573432803153992, 1.017...       1  \n",
       "2  [-0.25123998522758484, 0.3757574260234833, -0....       0  \n",
       "3  [0.7327960729598999, -0.7488707900047302, 0.55...       1  \n",
       "4  [-0.49589139223098755, -0.5760805606842041, 0....       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.tensor(batches[0].iloc[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the tensors and label from the dataframe\n",
    "        tensor1 = torch.tensor(self.dataframe.iloc[idx, 0].tolist(), dtype=torch.float32)\n",
    "        tensor2 = torch.tensor(self.dataframe.iloc[idx, 1].tolist(), dtype=torch.float32)\n",
    "        label = torch.tensor(self.dataframe.iloc[idx, 2].tolist(), dtype=torch.float32)\n",
    "        \n",
    "        return tensor1, tensor2, label\n",
    "\n",
    "# Load your dataframe\n",
    "# dataframe = pd.read_csv('../data/train/siamence_main_pic.parquet')\n",
    "train_dataframe = batches[0]\n",
    "test_dataframe = batches[1]\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "train_dataset = SiameseDataset(train_dataframe)\n",
    "test_dataset = SiameseDataset(test_dataframe)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiamenceNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiamenceNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(320,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        outpu1 = self.forward_one(input1)\n",
    "        outpu2 = self.forward_one(input2)\n",
    "        return outpu1, outpu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Compute Euclidean distance\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        loss = torch.mean(\n",
    "            (label) * torch.pow(euclidean_distance, 2) +  # Similar pairs: distance squared\n",
    "            (1 - label) * torch.pow(F.relu(self.margin - euclidean_distance), 2)  # Dissimilar pairs: margin - distance squared\n",
    "        )\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device - cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device - {device}')\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SiamenceNetwork().to(device)\n",
    "criterion = ContrastiveLoss().to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs, CE=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0  # To keep track of epoch loss\n",
    "        for tensor1, tensor2, label in dataloader:\n",
    "            # Move data to GPU\n",
    "            tensor1 = tensor1.to(device)\n",
    "            tensor2 = tensor2.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            if CE:\n",
    "                output = model(tensor1, tensor2)\n",
    "                loss = criterion(output, label.unsqueeze(1))\n",
    "            else:\n",
    "                output1, output2 = model(tensor1, tensor2)\n",
    "                loss = criterion(output1, output2, label)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * tensor1.size(0)  # Accumulate loss\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= len(dataloader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 0.1989\n",
      "Epoch [2/4], Loss: 0.1655\n",
      "Epoch [3/4], Loss: 0.1532\n",
      "Epoch [4/4], Loss: 0.1454\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "train_model(model, train_dataloader, criterion, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "def test_model_with_accuracy(model, dataloader, criterion, device, threshold=0.5, CE=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    true_values = []\n",
    "    predicted_values = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tensor1, tensor2, label in dataloader:\n",
    "            tensor1 = tensor1.to(device)\n",
    "            tensor2 = tensor2.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            if CE:\n",
    "                output = model(tensor1, tensor2)\n",
    "                loss = criterion(output, label.unsqueeze(1))\n",
    "                predictions = (output < threshold).float()\n",
    "            else:\n",
    "                output1, output2 = model(tensor1, tensor2)\n",
    "                loss = criterion(output1, output2, label)\n",
    "                euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                predictions = (euclidean_distance < threshold).float()\n",
    "            \n",
    "            total_loss += loss.item() * tensor1.size(0)  # Accumulate loss\n",
    "            num_samples += tensor1.size(0)\n",
    "            \n",
    "            # Move to CPU and detach before appending to lists\n",
    "            predicted_values.extend(predictions.cpu().detach().numpy())\n",
    "            true_values.extend(label.cpu().detach().numpy())\n",
    "            \n",
    "    \n",
    "    true_values = np.array(true_values)\n",
    "    predicted_values = np.array(predicted_values)\n",
    "    \n",
    "    average_loss = total_loss / num_samples\n",
    "    accuracy = accuracy_score(true_values, predicted_values)\n",
    "    recall = recall_score(true_values, predicted_values)\n",
    "    f1 = f1_score(true_values, predicted_values)\n",
    "    return average_loss, accuracy, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.3000, Loss: 0.1612, Accuracy: 0.7516, Recall: 0.5670, f1: 0.6873, \n",
      "Threshold: 0.4000, Loss: 0.1612, Accuracy: 0.7821, Recall: 0.7106, f1: 0.7585, \n",
      "Threshold: 0.5000, Loss: 0.1612, Accuracy: 0.7791, Recall: 0.8074, f1: 0.7788, \n",
      "Threshold: 0.6000, Loss: 0.1612, Accuracy: 0.7567, Recall: 0.8741, f1: 0.7758, \n",
      "Threshold: 0.7000, Loss: 0.1612, Accuracy: 0.7231, Recall: 0.9173, f1: 0.7613, \n"
     ]
    }
   ],
   "source": [
    "thresholds = []\n",
    "losses = []\n",
    "accuracies = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "threshold = 0\n",
    "for i in range(3, 8):\n",
    "    threshold = i*0.1\n",
    "    test_loss, test_accuracy, test_recall, test_f1 = test_model_with_accuracy(model, test_dataloader, criterion, device, threshold=threshold)\n",
    "    thresholds.append(threshold)\n",
    "    losses.append(test_loss)\n",
    "    accuracies.append(test_accuracy)\n",
    "    recalls.append(test_recall)\n",
    "    f1s.append(f1s)\n",
    "    print(f'Threshold: {threshold:.4f}, Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Recall: {test_recall:.4f}, f1: {test_f1:.4f}, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewNetwork(nn.Module):\n",
    "    def __init__(self, pretrained_fc):\n",
    "        super(NewNetwork, self).__init__()\n",
    "        # Use the pre-trained fully connected layers\n",
    "        self.fc = pretrained_fc\n",
    "        \n",
    "        # New layers after concatenation\n",
    "        self.concat_fc = nn.Sequential(\n",
    "            nn.Linear(20, 64),  # 10 + 10 = 20\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Get outputs from the pretrained fc layers\n",
    "        output1 = self.fc(input1)\n",
    "        output2 = self.fc(input2)\n",
    "        \n",
    "        # Concatenate the two outputs along the feature dimension\n",
    "        concat_output = torch.cat((output1, output2), dim=1)\n",
    "        \n",
    "        # Pass the concatenated output through the new layers\n",
    "        final_output = self.concat_fc(concat_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "new_model = NewNetwork(model.fc)\n",
    "new_model = new_model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "# Optionally freeze the pretrained layers\n",
    "for param in new_model.fc.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4923\n",
      "Epoch [2/10], Loss: 0.4253\n",
      "Epoch [3/10], Loss: 0.4171\n",
      "Epoch [4/10], Loss: 0.4127\n",
      "Epoch [5/10], Loss: 0.4097\n",
      "Epoch [6/10], Loss: 0.4091\n",
      "Epoch [7/10], Loss: 0.4075\n",
      "Epoch [8/10], Loss: 0.4067\n",
      "Epoch [9/10], Loss: 0.4052\n",
      "Epoch [10/10], Loss: 0.4038\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "train_model(new_model, train_dataloader, criterion, optimizer, device, num_epochs, CE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.3000, Loss: 0.4799, Accuracy: 0.2409, Recall: 0.1310, f1: 0.1426, \n",
      "Threshold: 0.4000, Loss: 0.4799, Accuracy: 0.2248, Recall: 0.1823, f1: 0.1847, \n",
      "Threshold: 0.5000, Loss: 0.4799, Accuracy: 0.2184, Recall: 0.2370, f1: 0.2260, \n",
      "Threshold: 0.6000, Loss: 0.4799, Accuracy: 0.2219, Recall: 0.2963, f1: 0.2683, \n",
      "Threshold: 0.7000, Loss: 0.4799, Accuracy: 0.2347, Recall: 0.3649, f1: 0.3147, \n"
     ]
    }
   ],
   "source": [
    "thresholds = []\n",
    "losses = []\n",
    "accuracies = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "threshold = 0\n",
    "for i in range(3, 8):\n",
    "    threshold = i*0.1\n",
    "    test_loss, test_accuracy, test_recall, test_f1 = test_model_with_accuracy(new_model, test_dataloader, criterion, device, threshold=threshold, CE=True)\n",
    "    thresholds.append(threshold)\n",
    "    losses.append(test_loss)\n",
    "    accuracies.append(test_accuracy)\n",
    "    recalls.append(test_recall)\n",
    "    f1s.append(f1s)\n",
    "    print(f'Threshold: {threshold:.4f}, Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Recall: {test_recall:.4f}, f1: {test_f1:.4f}, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(thresholds, f1s, label='F1 score', color='blue')\n",
    "# plt.plot(thresholds, accuracies, label='Accuracy', color='red')\n",
    "# plt.plot(thresholds, recalls, label='Recall', color='greed')\n",
    "\n",
    "# plt.xlabel('Threshold')\n",
    "# plt.ylabel('Score')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
