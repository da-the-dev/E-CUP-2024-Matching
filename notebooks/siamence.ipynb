{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_in_batches(file_path: str, batch_size=100000):\n",
    "\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    processed_rows = 0\n",
    "\n",
    "    for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "        batch_df = batch.to_pandas()\n",
    "\n",
    "        processed_rows += len(batch_df)\n",
    "        progress = (processed_rows / total_rows) * 100\n",
    "        print(f'Progress: {progress:.2f}%')\n",
    "        \n",
    "        yield batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 8.56%\n",
      "Progress: 17.12%\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "batches = []\n",
    "for batch in read_parquet_in_batches(file_path='../data/train/siamence_main_pic.parquet'):\n",
    "    if not batches:\n",
    "        batches.append(batch)\n",
    "    else:\n",
    "        batches.append(batch)\n",
    "        break\n",
    "    \n",
    "for batch in batches:\n",
    "    batch.drop(columns=['variantid1', 'variantid2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the tensors and label from the dataframe\n",
    "        tensor1 = torch.tensor(self.dataframe.iloc[idx, 1].tolist(), dtype=torch.float32)\n",
    "        tensor2 = torch.tensor(self.dataframe.iloc[idx, 2].tolist(), dtype=torch.float32)\n",
    "        label = torch.tensor(self.dataframe.iloc[idx, 0].tolist(), dtype=torch.float32)\n",
    "        \n",
    "        return tensor1, tensor2, label\n",
    "\n",
    "# Load your dataframe\n",
    "# dataframe = pd.read_csv('../data/train/siamence_main_pic.parquet')\n",
    "train_dataframe = batches[0]\n",
    "test_dataframe = batches[1]\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "train_dataset = SiameseDataset(train_dataframe)\n",
    "test_dataset = SiameseDataset(test_dataframe)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiamenceNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiamenceNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        outpu1 = self.forward_one(input1)\n",
    "        outpu2 = self.forward_one(input2)\n",
    "        return outpu1, outpu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Compute Euclidean distance\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        loss = torch.mean(\n",
    "            (label) * torch.pow(euclidean_distance, 2) +  # Similar pairs: distance squared\n",
    "            (1 - label) * torch.pow(F.relu(self.margin - euclidean_distance), 2)  # Dissimilar pairs: margin - distance squared\n",
    "        )\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device - cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device - {device}')\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SiamenceNetwork().to(device)\n",
    "criterion = ContrastiveLoss().to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.4117\n",
      "Epoch [2/100], Loss: 0.4028\n",
      "Epoch [3/100], Loss: 0.3997\n",
      "Epoch [4/100], Loss: 0.3974\n",
      "Epoch [5/100], Loss: 0.3956\n",
      "Epoch [6/100], Loss: 0.3946\n",
      "Epoch [7/100], Loss: 0.3937\n",
      "Epoch [8/100], Loss: 0.3930\n",
      "Epoch [9/100], Loss: 0.3920\n",
      "Epoch [10/100], Loss: 0.3917\n",
      "Epoch [11/100], Loss: 0.3912\n",
      "Epoch [12/100], Loss: 0.3906\n",
      "Epoch [13/100], Loss: 0.3897\n",
      "Epoch [14/100], Loss: 0.3899\n",
      "Epoch [15/100], Loss: 0.3893\n",
      "Epoch [16/100], Loss: 0.3888\n",
      "Epoch [17/100], Loss: 0.3884\n",
      "Epoch [18/100], Loss: 0.3885\n",
      "Epoch [19/100], Loss: 0.3885\n",
      "Epoch [20/100], Loss: 0.3877\n",
      "Epoch [21/100], Loss: 0.3873\n",
      "Epoch [22/100], Loss: 0.3873\n",
      "Epoch [23/100], Loss: 0.3871\n",
      "Epoch [24/100], Loss: 0.3868\n",
      "Epoch [25/100], Loss: 0.3865\n",
      "Epoch [26/100], Loss: 0.3861\n",
      "Epoch [27/100], Loss: 0.3859\n",
      "Epoch [28/100], Loss: 0.3858\n",
      "Epoch [29/100], Loss: 0.3852\n",
      "Epoch [30/100], Loss: 0.3852\n",
      "Epoch [31/100], Loss: 0.3849\n",
      "Epoch [32/100], Loss: 0.3842\n",
      "Epoch [33/100], Loss: 0.3842\n",
      "Epoch [34/100], Loss: 0.3840\n",
      "Epoch [35/100], Loss: 0.3837\n",
      "Epoch [36/100], Loss: 0.3834\n",
      "Epoch [37/100], Loss: 0.3837\n",
      "Epoch [38/100], Loss: 0.3835\n",
      "Epoch [39/100], Loss: 0.3832\n",
      "Epoch [40/100], Loss: 0.3830\n",
      "Epoch [41/100], Loss: 0.3826\n",
      "Epoch [42/100], Loss: 0.3823\n",
      "Epoch [43/100], Loss: 0.3821\n",
      "Epoch [44/100], Loss: 0.3817\n",
      "Epoch [45/100], Loss: 0.3816\n",
      "Epoch [46/100], Loss: 0.3813\n",
      "Epoch [47/100], Loss: 0.3817\n",
      "Epoch [48/100], Loss: 0.3813\n",
      "Epoch [49/100], Loss: 0.3812\n",
      "Epoch [50/100], Loss: 0.3807\n",
      "Epoch [51/100], Loss: 0.3809\n",
      "Epoch [52/100], Loss: 0.3806\n",
      "Epoch [53/100], Loss: 0.3806\n",
      "Epoch [54/100], Loss: 0.3808\n",
      "Epoch [55/100], Loss: 0.3807\n",
      "Epoch [56/100], Loss: 0.3800\n",
      "Epoch [57/100], Loss: 0.3803\n",
      "Epoch [58/100], Loss: 0.3803\n",
      "Epoch [59/100], Loss: 0.3801\n",
      "Epoch [60/100], Loss: 0.3799\n",
      "Epoch [61/100], Loss: 0.3801\n",
      "Epoch [62/100], Loss: 0.3801\n",
      "Epoch [63/100], Loss: 0.3800\n",
      "Epoch [64/100], Loss: 0.3798\n",
      "Epoch [65/100], Loss: 0.3800\n",
      "Epoch [66/100], Loss: 0.3797\n",
      "Epoch [67/100], Loss: 0.3799\n",
      "Epoch [68/100], Loss: 0.3799\n",
      "Epoch [69/100], Loss: 0.3800\n",
      "Epoch [70/100], Loss: 0.3795\n",
      "Epoch [71/100], Loss: 0.3796\n",
      "Epoch [72/100], Loss: 0.3797\n",
      "Epoch [73/100], Loss: 0.3794\n",
      "Epoch [74/100], Loss: 0.3792\n",
      "Epoch [75/100], Loss: 0.3792\n",
      "Epoch [76/100], Loss: 0.3791\n",
      "Epoch [77/100], Loss: 0.3792\n",
      "Epoch [78/100], Loss: 0.3796\n",
      "Epoch [79/100], Loss: 0.3790\n",
      "Epoch [80/100], Loss: 0.3791\n",
      "Epoch [81/100], Loss: 0.3792\n",
      "Epoch [82/100], Loss: 0.3794\n",
      "Epoch [83/100], Loss: 0.3790\n",
      "Epoch [84/100], Loss: 0.3787\n",
      "Epoch [85/100], Loss: 0.3786\n",
      "Epoch [86/100], Loss: 0.3792\n",
      "Epoch [87/100], Loss: 0.3785\n",
      "Epoch [88/100], Loss: 0.3787\n",
      "Epoch [89/100], Loss: 0.3786\n",
      "Epoch [90/100], Loss: 0.3788\n",
      "Epoch [91/100], Loss: 0.3787\n",
      "Epoch [92/100], Loss: 0.3786\n",
      "Epoch [93/100], Loss: 0.3786\n",
      "Epoch [94/100], Loss: 0.3787\n",
      "Epoch [95/100], Loss: 0.3786\n",
      "Epoch [96/100], Loss: 0.3784\n",
      "Epoch [97/100], Loss: 0.3785\n",
      "Epoch [98/100], Loss: 0.3783\n",
      "Epoch [99/100], Loss: 0.3785\n",
      "Epoch [100/100], Loss: 0.3785\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0  # To keep track of epoch loss\n",
    "    for tensor1, tensor2, label in train_dataloader:\n",
    "        # Move data to GPU\n",
    "        tensor1 = tensor1.to(device)\n",
    "        tensor2 = tensor2.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output1, output2 = model(tensor1, tensor2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output1, output2, label)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * tensor1.size(0)  # Accumulate loss\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.0000, Loss: 0.3784, Accuracy: 0.5185, Recall: 0.0000, f1: 0.0000, \n",
      "Threshold: 0.1000, Loss: 0.3784, Accuracy: 0.7109, Recall: 0.6450, f1: 0.6824, \n",
      "Threshold: 0.2000, Loss: 0.3784, Accuracy: 0.7192, Recall: 0.7012, f1: 0.7063, \n",
      "Threshold: 0.3000, Loss: 0.3781, Accuracy: 0.7142, Recall: 0.7569, f1: 0.7183, \n",
      "Threshold: 0.4000, Loss: 0.3783, Accuracy: 0.6866, Recall: 0.8250, f1: 0.7171, \n",
      "Threshold: 0.5000, Loss: 0.3782, Accuracy: 0.6313, Recall: 0.8939, f1: 0.7001, \n",
      "Threshold: 0.6000, Loss: 0.3781, Accuracy: 0.5707, Recall: 0.9487, f1: 0.6803, \n",
      "Threshold: 0.7000, Loss: 0.3781, Accuracy: 0.5225, Recall: 0.9795, f1: 0.6639, \n",
      "Threshold: 0.8000, Loss: 0.3784, Accuracy: 0.4970, Recall: 0.9929, f1: 0.6553, \n",
      "Threshold: 0.9000, Loss: 0.3785, Accuracy: 0.4863, Recall: 0.9981, f1: 0.6517, \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "def test_model_with_accuracy(model, dataloader, criterion, device, threshold=0.5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    true_values = []\n",
    "    predicted_values = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for tensor1, tensor2, label in dataloader:\n",
    "            tensor1 = tensor1.to(device)\n",
    "            tensor2 = tensor2.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1, output2 = model(tensor1, tensor2)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output1, output2, label)\n",
    "            total_loss += loss.item() * tensor1.size(0)  # Accumulate loss\n",
    "            num_samples += tensor1.size(0)\n",
    "\n",
    "            # Compute predictions\n",
    "            euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "            predictions = (euclidean_distance < threshold).float()\n",
    "            \n",
    "            # Move to CPU and detach before appending to lists\n",
    "            predicted_values.extend(predictions.cpu().detach().numpy())\n",
    "            true_values.extend(label.cpu().detach().numpy())\n",
    "            \n",
    "    \n",
    "    true_values = np.array(true_values)\n",
    "    predicted_values = np.array(predicted_values)\n",
    "    \n",
    "    average_loss = total_loss / num_samples\n",
    "    accuracy = accuracy_score(true_values, predicted_values)\n",
    "    recall = recall_score(true_values, predicted_values)\n",
    "    f1 = f1_score(true_values, predicted_values)\n",
    "    return average_loss, accuracy, recall, f1\n",
    "\n",
    "# Example usage\n",
    "thresholds = []\n",
    "losses = []\n",
    "accuracies = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "threshold = 0\n",
    "for i in range(0, 10):\n",
    "    threshold = i*0.1\n",
    "    test_loss, test_accuracy, test_recall, test_f1 = test_model_with_accuracy(model, test_dataloader, criterion, device, threshold=threshold)\n",
    "    thresholds.append(threshold)\n",
    "    losses.append(test_loss)\n",
    "    accuracies.append(test_accuracy)\n",
    "    recalls.append(test_recall)\n",
    "    f1s.append(f1s)\n",
    "    print(f'Threshold: {threshold:.4f}, Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Recall: {test_recall:.4f}, f1: {test_f1:.4f}, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'thresholds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mthresholds\u001b[49m, f1s, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 score\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(thresholds, accuracies, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(thresholds, recalls, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'thresholds' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(thresholds, f1s, label='F1 score', color='blue')\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', color='red')\n",
    "plt.plot(thresholds, recalls, label='Recall', color='greed')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
